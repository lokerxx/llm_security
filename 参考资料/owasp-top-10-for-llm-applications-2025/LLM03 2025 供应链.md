# LLM03:2025 供应链

## 描述

LLM供应链面临多种漏洞，这些漏洞可能影响训练数据、模型和部署平台的完整性。这些风险可能导致偏差输出、安全漏洞或系统故障。与传统软件漏洞（如代码缺陷和依赖关系）不同，ML的风险还涉及第三方预训练模型和数据，这些外部元素可能会通过篡改或中毒攻击被操控。

LLM的创建是一项专业任务，通常依赖于第三方模型。开源LLM的兴起以及“LoRA”（低秩适配）和“PEFT”（参数高效微调）等新型微调方法，尤其是在Hugging Face等平台上的应用，带来了新的供应链风险。此外，设备端LLM的出现扩大了攻击面，增加了LLM应用程序的供应链风险。

本文讨论的部分风险也在“LLM04 数据和模型中毒”中提到，此处更关注风险的供应链方面。  
简单威胁模型参考：[A06:2021 – Vulnerable and Outdated Components](https://owasp.org/API-Security/editions/2021/en/).

---

## 常见风险示例

1. **传统的第三方包漏洞**  
   如过时或已弃用的组件，攻击者可能利用这些漏洞破坏LLM应用程序。这类似于“A06:2021 – 易受攻击和过时的组件”，但在模型开发或微调时使用这些组件会增加风险。

2. **许可证风险**  
   AI开发通常涉及多样化的软件和数据集许可证，管理不当可能带来风险。不同的开源和专有许可证对使用、分发和商业化施加不同的法律要求。

3. **过时或已弃用的模型**  
   使用不再维护的过时或弃用模型会导致安全问题。

4. **易受攻击的预训练模型**  
   模型是二进制黑盒，与开源软件不同，静态检查很难提供安全保障。易受攻击的预训练模型可能包含未通过模型库安全评估识别的隐藏偏差、后门或其他恶意功能。  
   易受攻击的模型可能因被污染的数据集或直接模型篡改技术（如ROME，也称为“脑叶切除术”）而产生。

5. **弱模型来源保证**  
   目前已发布的模型缺乏强有力的来源保证。模型卡和相关文档虽提供信息，但不能保证模型来源。攻击者可能通过社会工程技术，利用模型仓库中的供应商账户漏洞或创建相似账户，从而危害LLM应用程序的供应链。

6. **易受攻击的LoRA适配器**  
   LoRA作为一种流行的微调技术，通过允许将预训练层添加到现有LLM中提高模块化能力。然而，这种方法带来了新的风险，例如恶意LoRA适配器可能破坏预训练基础模型的完整性和安全性。  
   此问题可能出现在协作模型合并环境中，也可能通过支持LoRA的流行推理部署平台（如vLMM和OpenLLM）下载并应用适配器到已部署的模型中。

7. **利用协作开发过程**  
   在共享环境中托管的协作模型合并和模型处理服务（如转换）可能被利用，在共享模型中引入漏洞。模型合并在Hugging Face等平台上非常流行，攻击者可能通过这种方式绕过审查。此外，聊天机器人等服务也被证明易受操控，可能在模型中引入恶意代码。

8. **设备端LLM供应链漏洞**  
   设备端LLM模型的供应链攻击面更广，可能因为制造过程被破坏或设备操作系统或固件漏洞被利用而受到影响。攻击者可以通过逆向工程重新打包应用程序并篡改模型。

9. **不明确的条款和数据隐私政策**  
   模型运营商的不明确条款和数据隐私政策可能导致应用程序的敏感数据被用于模型训练，从而暴露敏感信息。此外，使用由模型供应商提供的受版权保护的材料也可能带来风险。

---

## 参考链接

1. **A06:2021 – 易受攻击和过时的组件**  
   [参考链接](https://owasp.org/API-Security/editions/2021/en/)

## 预防和缓解策略

1. **审查数据源和供应商**  
   仔细审查数据源和供应商，包括条款和条件（T&Cs）及其隐私政策，仅使用可信供应商。定期审查和审计供应商的安全性和访问控制，确保其安全状况或T&Cs未发生变化。

2. **遵循OWASP Top Ten中的“A06:2021 – 易受攻击和过时的组件”缓解措施**  
   包括漏洞扫描、管理和修补组件。在访问敏感数据的开发环境中也需应用这些控制措施。  
   **参考链接**: [A06:2021 – 易受攻击和过时的组件](https://owasp.org/API-Security/editions/2021/en/)

3. **使用全面的AI红队评估**  
   在选择第三方模型时进行全面的AI红队测试。“Decoding Trust”是LLM值得信赖的AI基准的一个示例，但模型可能通过微调绕过已发布的基准。使用广泛的AI红队测试评估模型，特别是在您计划使用的用例中。

4. **维护最新的组件清单**  
   使用软件物料清单（SBOM）保持最新、准确且签名的组件清单，以防止部署包被篡改。SBOM可用于快速检测和警报新的“零日漏洞”。AI BOMs和ML SBOMs是一个新兴领域，建议从OWASP CycloneDX开始评估选项。

5. **管理AI许可证风险**  
   使用BOM创建涉及的所有许可证类型的清单，并定期审核所有软件、工具和数据集，确保通过BOM实现合规性和透明性。使用自动化许可证管理工具进行实时监控，并对团队进行许可证模型培训。将详细的许可证文档维护在BOM中。

6. **仅使用可验证来源的模型**  
   使用带有签名和文件哈希的第三方模型完整性检查，以弥补模型来源不足的问题。同样，对外部提供的代码使用代码签名。

7. **实施严格的监控和审计实践**  
   在协作模型开发环境中，防止并快速检测滥用行为。例如，可以使用自动化脚本，如“HuggingFace SF_Convertbot Scanner”。  
   **参考链接**: [HuggingFace SF_Convertbot Scanner](https://huggingface.co/)

8. **检测模型和数据篡改**  
   对提供的模型和数据进行异常检测和对抗鲁棒性测试，以检测篡改和中毒攻击，如“LLM04 数据和模型中毒”中讨论的内容。这些技术可作为红队测试的一部分，或在MLOps和LLM管道中实施。

9. **实施修补政策**  
   缓解易受攻击或过时的组件风险。确保应用程序依赖于受维护的API和基础模型版本。

10. **加密边缘部署模型并进行完整性检查**  
      对AI边缘部署的模型进行加密，添加完整性检查，并使用供应商认证API，防止篡改的应用程序和模型。终止无法识别的固件的应用程序运行。

## 示例攻击场景

### 场景 #1: **易受攻击的Python库**  
攻击者利用一个易受攻击的Python库攻击LLM应用程序。这曾发生在OpenAI的首次数据泄露中，攻击者通过PyPi包注册表诱骗模型开发人员下载带有恶意软件的PyTorch依赖包，从而在模型开发环境中实施攻击。一个更复杂的例子是针对Ray AI框架的“Shadow Ray”攻击，据称有五个漏洞在真实环境中被利用，影响了许多服务器。

---

### 场景 #2: **直接篡改**  
攻击者通过直接篡改模型参数，并发布一个模型来传播错误信息。这种攻击在实际中发生过，例如“PoisonGPT”绕过了Hugging Face的安全功能，直接修改了模型参数。

---

### 场景 #3: **微调流行模型**  
攻击者微调一个流行的开放访问模型，移除关键安全功能，并针对特定领域（如保险）表现优异。模型被微调以在安全基准测试中得高分，但设有针对性的触发条件。攻击者将其部署在Hugging Face上，利用受害者对基准测试的信任。

---

### 场景 #4: **预训练模型**  
一个LLM系统从广泛使用的模型库中部署预训练模型，但没有进行彻底验证。一个被篡改的模型引入了恶意代码，在某些情况下导致偏差输出，进而引发有害或被操控的结果。

---

### 场景 #5: **受损的第三方供应商**  
一个受损的第三方供应商提供了一个漏洞百出的LoRA适配器，并通过Hugging Face上的模型合并功能将其合并到LLM中。

---

### 场景 #6: **供应商渗透**  
攻击者渗透一个第三方供应商，篡改生产的LoRA适配器，使其包含隐藏漏洞和恶意代码。当适配器与LLM合并后，攻击者通过恶意代码获得系统的隐秘入口点，操控模型输出。

---

### 场景 #7: **云攻击（CloudBorne 和 CloudJacking）**  
这些攻击目标是云基础设施，通过利用虚拟化层的共享资源和漏洞进行攻击。  
- **CloudBorne**: 利用共享云环境中的固件漏洞，破坏托管虚拟实例的物理服务器。  
- **CloudJacking**: 恶意控制或滥用云实例，可能导致对关键LLM部署平台的未经授权访问。

---

### 场景 #8: **LeftOvers（CVE-2023-4969）**  
攻击者利用GPU本地内存泄漏漏洞恢复敏感数据。这种攻击可以用来从生产服务器、开发工作站或笔记本中窃取敏感数据。

---

### 场景 #9: **WizardLM假模型**  
在WizardLM模型被移除后，攻击者利用用户对该模型的兴趣，发布带有相同名称但包含恶意软件和后门的假模型。

---

### 场景 #10: **模型合并/格式转换服务**  
攻击者通过模型合并或格式转换服务实施攻击，破坏一个公开访问的模型并注入恶意软件。这种攻击由供应商HiddenLayer披露过。

---

### 场景 #11: **逆向工程移动应用**  
攻击者通过逆向工程移动应用程序，替换其中的模型为篡改版本，诱导用户访问诈骗网站。用户被鼓励通过社交工程技术直接下载该应用。这种攻击实际发生在116个Google Play应用上，包括许多安全和关键应用，如现金识别、家长控制、面部认证和金融服务。  
**参考链接**: [真实预测性AI攻击案例](https://example.com)

---

### 场景 #12: **数据集中毒**  
攻击者中毒公开可用的数据集，创建后门，用于微调模型时对某些公司在不同市场中产生偏好。

---

### 场景 #13: **T&Cs和隐私政策更改**  
LLM运营商更改其T&Cs和隐私政策，要求用户显式选择退出将应用程序数据用于模型训练。这可能导致模型记忆敏感数据。