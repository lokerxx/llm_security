# LLM09:2025 错误信息

## 描述

LLM生成的错误信息对依赖这些模型的应用程序构成核心漏洞。当LLM生成看似可信但实际上虚假的或误导性的信息时，就会出现错误信息。这种漏洞可能导致安全漏洞、声誉损害以及法律责任。

**错误信息的主要原因之一是“幻觉”**，即LLM生成的内容看似正确但实为虚构。幻觉发生在LLM填补其训练数据中的空白时，仅依据统计模式而非真正理解内容。因此，模型可能生成听起来合理但完全没有依据的答案。除了幻觉，训练数据中的偏见和信息不完整也可能导致错误信息。

**过度依赖**是一个相关问题，即用户对LLM生成的内容过于信任，未能验证其准确性。这种过度依赖加剧了错误信息的影响，因为用户可能在关键决策或流程中使用未经充分审查的错误数据。

---

## 常见风险示例

1. **事实错误**  
   模型生成错误陈述，导致用户基于虚假信息做出决策。例如，加拿大航空的聊天机器人向旅客提供错误信息，造成运营中断和法律纠纷，最终被成功起诉。  
   **参考链接**: [BBC](https://www.bbc.com)

2. **无依据的声明**  
   模型生成毫无根据的断言，在敏感领域（如医疗或法律）尤为有害。例如，ChatGPT虚构法律案例，导致法庭上的重大问题。  
   **参考链接**: [LegalDive](https://www.legaldive.com)

3. **虚假专业知识**  
   模型给人一种理解复杂主题的假象，误导用户对其专业水平的信任。例如，某些聊天机器人错误地表示健康问题存在不确定性，导致用户误以为未获支持的治疗方法仍在讨论中。  
   **参考链接**: [KFF](https://www.kff.org)

4. **不安全的代码生成**  
   模型建议使用不安全或不存在的代码库，这可能在集成到软件系统中时引入漏洞。例如，LLM提议使用不安全的第三方库，开发者未经验证便信任这些建议，导致安全风险。  
   **参考链接**: [Lasso](https://www.lasso.com)

---

## 预防与缓解策略

1. **检索增强生成（RAG）**  
   使用RAG通过从可信的外部数据库中检索相关且经过验证的信息来生成响应，增强模型输出的可靠性，降低幻觉和错误信息的风险。

2. **模型微调**  
   通过微调或嵌入技术提升模型输出质量。使用参数高效调优（PET）和链式思维提示（Chain-of-Thought Prompting）等技术可减少错误信息的发生。

3. **交叉验证与人工监督**  
   鼓励用户与可信外部来源交叉检查LLM输出，确保信息准确性。为关键或敏感信息实施人工监督和事实核查流程，确保人工审查员接受适当培训，避免对AI生成内容的过度依赖。

4. **自动验证机制**  
   在高风险环境中，使用工具和流程自动验证关键输出。

5. **风险沟通**  
   明确告知用户LLM生成内容的风险和局限性，包括可能产生的错误信息。

6. **安全编码实践**  
   建立安全编码实践，防止因错误的代码建议而引入漏洞。

7. **用户界面设计**  
   设计鼓励负责任使用LLM的API和用户界面，例如集成内容过滤器，清晰标注AI生成的内容，并告知用户其可靠性和准确性的局限性。

8. **培训与教育**  
   为用户提供LLM局限性、生成内容独立验证的重要性以及批判性思维的全面培训。在特定环境下，提供与领域相关的专业培训。

---

## 示例攻击场景

### 场景 #1: **恶意代码库的利用**  
攻击者利用流行的代码助手，找出常见的幻觉生成的包名称。发现这些不存在的库后，攻击者将恶意包发布到流行的代码库中。开发者信任代码助手的建议，无意间将这些恶意包集成到软件中。结果，攻击者获得了未授权访问权限，注入恶意代码或建立后门，导致重大安全漏洞并危及用户数据。

### 场景 #2: **医疗错误信息的后果**  
某公司提供的医疗诊断聊天机器人未确保足够的准确性。机器人提供错误信息，导致患者出现不良后果。公司因损害成功被起诉。在这种情况下，即使没有恶意攻击者，由于LLM系统的监督不足和可靠性问题，公司仍面临声誉和财务风险。