# LLM02:2025 敏感信息泄露

## 描述

敏感信息可能会影响LLM及其应用场景，包括个人身份信息（PII）、财务信息、健康记录、机密商业数据、安全凭证和法律文件。对于封闭或基础模型而言，专有的训练方法和源代码也被视为敏感信息。

当LLM嵌入到应用程序中时，存在通过输出暴露敏感数据、专有算法或机密信息的风险。这可能导致未经授权的数据访问、隐私泄露以及知识产权侵犯。用户需要了解如何与LLM安全交互，避免无意间提供敏感数据，以免这些数据稍后在模型输出中被披露。

为降低此风险，LLM应用程序应实施适当的数据清理措施，防止用户数据进入训练模型。应用所有者应提供明确的使用条款政策，允许用户选择不将其数据用于训练模型。此外，在系统提示中限制模型返回的数据类型也可以在一定程度上缓解敏感信息泄露，但这些限制可能会被提示注入或其他方法绕过。

---

## 常见漏洞示例

1. **PII 泄露** 
   个人身份信息（PII）可能在与LLM交互时被披露。

2. **专有算法暴露** 
   配置不当的模型输出可能会泄露专有算法或数据。披露的训练数据可能引发反转攻击（如“Proof Pudding”攻击，CVE-2019-20634），攻击者能够提取敏感信息或重建输入内容，从而绕过安全控制或规避机器学习算法中的电子邮件过滤器。

3. **敏感商业数据泄露** 
   生成的响应可能无意中包含机密的商业信息。

---

## 预防和缓解策略

### 数据清理
1. **集成数据清理技术** 
   实施数据清理，防止用户数据进入训练模型，包括清除或屏蔽敏感内容。

2. **健全的输入验证** 
   应用严格的输入验证方法，检测并过滤潜在有害或敏感的数据输入，确保它们不会影响模型安全。

---

### 访问控制
1. **强制严格的访问控制** 
   根据最小权限原则限制对敏感数据的访问，仅允许特定用户或过程访问必要数据。

2. **限制数据源** 
   限制模型对外部数据源的访问，并确保运行时数据编排的安全管理，以避免意外的数据泄露。

---

### 联邦学习和隐私技术
1. **利用联邦学习** 
   使用分布式数据在多个服务器或设备上训练模型。这种方法减少了集中式数据收集的需求，并降低了风险暴露。

2. **引入差分隐私** 
   通过向数据或输出中添加噪声，阻止攻击者反向工程单个数据点。

---

### 用户教育与透明度
1. **教育用户安全使用LLM** 
   提供避免输入敏感信息的指导。培训用户与LLM安全交互的最佳实践。

2. **确保数据使用透明度** 
   维护清晰的数据保留、使用和删除政策，允许用户选择不将其数据用于训练过程。

---

### 安全系统配置
1. **隐藏系统前言** 
   限制用户覆盖或访问系统初始设置的能力，减少内部配置被暴露的风险。

2. **参考安全配置错误的最佳实践** 
   遵循例如“OWASP API8:2023 Security Misconfiguration”的指南，以防止通过错误消息或配置细节泄露敏感信息。 
   **参考链接**: [OWASP API8:2023 Security Misconfiguration](https://owasp.org/API-Security/editions/2023/en/)

---

### 高级技术
1. **同态加密** 
   使用同态加密实现安全数据分析和隐私保护的机器学习，确保数据在处理过程中保持机密。

2. **令牌化和信息编辑** 
   实施令牌化技术以预处理和清理敏感信息。通过模式匹配技术检测和编辑机密内容，防止处理时泄露。

---

## 攻击场景示例

### 场景 1: **无意数据暴露**  
用户收到的响应包含其他用户的个人数据，这是由于缺乏充分的数据清理导致的。

### 场景 2: **目标提示注入**  
攻击者绕过输入过滤器，提取敏感信息。

### 场景 3: **通过训练数据泄露数据**  
由于训练过程中数据包含疏忽，导致敏感信息被披露。
