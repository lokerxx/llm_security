# LLM01: 2025 提示注入漏洞

## 描述

**提示注入漏洞**是指用户的输入提示会以意想不到的方式改变大型语言模型（LLM）的行为或输出。这些输入即使对人类而言不可察觉，也可能影响模型，因此提示注入并不需要对人类可见或可读，只要内容能够被模型解析即可。

提示注入漏洞的存在是由于模型处理提示的方式，以及输入可能迫使模型错误地将提示数据传递给模型的其他部分。这可能导致模型违反指南、生成有害内容、启用未经授权的访问或影响关键决策。尽管检索增强生成（RAG）和微调等技术旨在使LLM的输出更相关和准确，但研究表明，它们无法完全消除提示注入漏洞。

提示注入与“越狱”是LLM安全领域的相关概念，二者常被混用。提示注入是通过特定输入操控模型响应以改变其行为，包括绕过安全措施。而“越狱”是一种提示注入，攻击者通过输入使模型完全忽略其安全协议。开发者可以通过系统提示和输入处理中的防护措施来减轻提示注入攻击，但有效防止“越狱”需要对模型训练和安全机制进行持续更新。

---

## 提示注入漏洞类型

### 1. 直接提示注入
**直接提示注入**是指用户输入的提示直接以意外或未预期的方式改变模型行为。这类输入可以是有意的（如恶意行为者故意设计的提示来利用模型），也可以是无意的（如用户无意中提供了触发意外行为的输入）。

### 2. 间接提示注入
**间接提示注入**是指当LLM接受来自外部来源的输入（如网站或文件）时，外部内容中可能包含数据，这些数据在被模型解析后会以意外或未预期的方式改变模型的行为。与直接注入类似，间接注入既可以是有意的，也可以是无意的。

---

## 提示注入攻击的影响

提示注入攻击的影响严重性和性质可能因模型所在的业务环境及其架构设计而大相径庭。但总体而言，提示注入可能导致以下结果（包括但不限于）：

- **敏感信息泄露**
- **暴露AI系统基础设施或系统提示的敏感信息**
- **内容操控，导致输出错误或存在偏见**
- **提供对LLM可用功能的未经授权访问**
- **在连接系统中执行任意命令**
- **操控关键决策过程**

---

## 多模态AI的特殊风险

随着同时处理多种数据类型的多模态AI的兴起，提示注入的风险也变得更加复杂。恶意行为者可能利用模态之间的交互来发起攻击，例如在伴随正常文本的图像中隐藏指令。这种系统的复杂性扩大了攻击面。多模态模型还可能容易受到难以检测和缓解的跨模态新型攻击。

为多模态系统设计强有力的防御措施是未来研究和开发的重要方向。



## 预防与缓解策略

**提示注入漏洞**是生成式AI本质带来的可能性。由于模型运作方式中存在随机性影响，目前尚不明确是否有万无一失的防御方法。然而，通过以下措施可以减轻提示注入的影响：

---

## 缓解措施

### 1. 限制模型行为
在系统提示中提供关于模型角色、能力和限制的明确说明。加强对上下文的严格遵循，限制模型的响应仅限于特定任务或主题，指示模型忽略修改核心指令的尝试。

### 2. 定义并验证期望的输出格式
明确指定输出格式，要求详细的推理和来源引用，使用确定性代码验证输出是否符合指定格式。

### 3. 实施输入和输出过滤
定义敏感内容类别，并构建识别和处理此类内容的规则。应用语义过滤器并使用字符串检查扫描非允许内容。通过RAG三角（上下文相关性、依据性、问答相关性）评估响应，识别潜在的恶意输出。

### 4. 执行权限控制和最小特权访问
为应用提供专属API令牌以扩展功能，并在代码中处理这些功能，而非交由模型处理。将模型的访问权限限制为其预期操作所需的最低权限。

### 5. 要求高风险操作的人为批准
为特权操作实施“人机协作”控制，以防止未经授权的操作。

### 6. 隔离并标识外部内容
分离并清楚标注不受信任的内容，限制其对用户提示的影响。

### 7. 进行对抗性测试和攻击模拟
定期执行渗透测试和漏洞模拟，将模型视为不受信任的用户，以测试信任边界和访问控制的有效性。

---

## 攻击场景示例

### 场景 1: **直接注入**
攻击者向客户支持聊天机器人注入提示，指示其忽略之前的指南、查询私有数据存储，并发送电子邮件，从而导致未经授权的访问和权限升级。

### 场景 2: **间接注入**
用户利用LLM总结包含隐藏指令的网页，这些指令导致LLM插入链接到特定URL的图片，泄露私人对话。

### 场景 3: **无意注入**
某公司在职位描述中加入指令以识别AI生成的申请。一名申请者在不知情的情况下使用LLM优化其简历，无意间触发AI检测。

### 场景 4: **有意模型影响**
攻击者修改了RAG应用程序使用的文档库中的内容。当用户的查询返回修改后的内容时，恶意指令改变了LLM的输出，生成误导性结果。

### 场景 5: **代码注入**
攻击者利用LLM驱动的电子邮件助手中的漏洞（CVE-2024-5184）注入恶意提示，获取敏感信息并操控电子邮件内容。

### 场景 6: **负载分割**
攻击者上传含有分割恶意提示的简历。当LLM用于评估候选人时，组合后的提示操控模型的响应，导致给出积极推荐，尽管简历内容并不符合要求。

### 场景 7: **多模态注入**
攻击者在伴随正常文本的图像中嵌入恶意提示。当多模态AI同时处理图像和文本时，隐藏提示改变模型行为，可能导致未经授权的操作或敏感信息的泄露。

### 场景 8: **对抗性后缀**
攻击者在提示后附加看似无意义的字符串，却以恶意方式影响LLM的输出，绕过安全措施。

### 场景 9: **多语言/混淆攻击**
攻击者使用多种语言或编码恶意指令（如Base64或表情符号）来规避过滤器并操控LLM行为。

